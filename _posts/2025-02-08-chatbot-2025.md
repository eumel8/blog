---
layout: post
tag: inet
title: KI Chatbots 2025 - der grosse Vergleich
subtitle: DeepSeek-R1 und Co auf dem Heimcomputer im Eigentest
date: 2025-02-08
author: eumel8
background: '/images/ai-chatbots4.png?4362984378'
twitter: 'images/blog-eumel-de.png?4362984378'
---

# Einstieg

Eines gleich vorweg: Ich bin nicht der grosse AI-Experte. Auch wenn ich [2018](https://blog.eumel.de/2018/12/29/2018-recap-a-year-around.html) schon mal an einem AI-Hackathon teilgenommen habe, und mich [2011](https://blog.eumel.de/2011/01/09/mein-neuer-freund-der-stringtokenizer.html) mit MapReduce und Tokenizer besch√§ftigte, bin ich weder beruflich noch privat in das Thema AI weiter eingestiegen. Das Thema schien zu weit weg zu sein, auch wenn es [abundzu](https://cognigy.com) doch Ber√ºhrungspunkte gab. ChatGPT von openAI hat alles ver√§ndert. KI ist pl√∂tzlich massentauglich geworden, quasi √ºber Nacht.
Jetzt kann man das auch noch autark betreiben. Schaut mal rein.

# Schnellergebnis

Wer den Artikel nicht bis zu Ende lesen m√∂chte, bekommt hier schon das Ergebnis: Eine richtige KI gibt es nicht. Das ist wie, als wenn man sich ein Auto kaufen m√∂chte, oder ein Haus. Die Geschm√§cker sind verschieden und auch die Bed√ºrfnisse eines jeden Menschen. Welche Anforderungen habe ich an eine KI? Seit DeepSeek-R1 versuchen Menschen Antworten zu aktuellen politischen Ereignissen zu finden oder vergangenen gesellschaftliche Ereignissen in China. Mag man spannend finden, aber man hat das Thema vielleicht noch nicht verstanden. Also doch der volle Exkurs.

# Large Language Model (LLM)

Ein Large Language Model (LLM) ist sowas wie ein Einweckglas voll mit Wissen. Quasi eine Enzyklop√§die mit allen m√∂glichen Informationen, zusammengequetscht und abgepackt. Dabei verstehen Computer gar keine Sprache, sie kennen nur die ber√ºhmten Nullen und Einsen, wie damals an der Lochstreifenstanze, wo alles begann. Mit dem Natural Language Processing (NLP) kann der Computer jetzt auch menschliche Sprachen verstehen. Also etwa bei einer Suchmaschine gebe ich einen Suchbegriff ein und nicht 11011. Mit dem  Deep Learning oder Machine Learning kann der Computer jetzt nicht nur nat√ºrliche, menschliche Sprachen verstehen, sondern kann diese interpretieren, mit seiner Datenmenge abgleichen und daraus R√ºckschl√ºsse ziehen. Das Ergebnis kann dann eine neue Erkenntnis sein, die der Computer uns mitteilt. Basis sind aber immer menschliche Informationen, die er irgendwann bekommen hat, bevor man ihn in das Einweckglas gesperrt hat.

# Auf die Gr√∂sse kommt es an

Es gibt verschiedene LLM und dasselbe LLM in verschiedenen Gr√∂ssen. Die Gr√∂sse beschreibt die Anzahl der Parameter, die im LLM verarbeitet wurden. Am Anfang waren das unendlich viele, sodass es gigantische Mengen an Grafikspeicher (GPU) bedurfte, um das Model √ºberhaupt zu starten. Mittlerweile ist das alles optimiert und verkleinert. Eine g√§ngige Gr√∂sse ist etwa `7B`, was f√ºr 7 Billion, also 7 Milliarden Parameter steht. So ein Model ist rund 5GB gross und l√§uft auf jedem handels√ºblichen Heim-PC.
Die Endausbaustufe enth√§lt nochmal 100 Mal mehr und damit steigt dann auch wieder der Hardwarebedarf. Auf Youtube gibt es einige Videos von Leuten, die mit [Hardware 671b](https://youtu.be/Tq_cmN4j2yY), das gr√∂sse Model von Deepseek-R1 starten.

Aber Obacht: Gr√∂sseres Model und mehr Rechenleistung bedeutet nicht automatisch, dass das LLM auch schneller ist. Die Antwortzeiten liegen teilweise im Minutenbereich. Wenn es darum geht, schnell dem Anwender etwas zu pr√§sentieren, gibt es Turbo-Modelle wie GPT-Turbo. Dort antworten die LLM nahezu in Echtzeit, allerdings liegen sie mit dem "Schnellschuss" auch mal daneben. Das ist wie bei spontanen Antworten beim Menschen gegen√ºber wohl√ºberlegten.

# Retrieval Augmented Generation (RAG)

Das LLM im Einweckglas weiss vielleicht sehr viel, ist aber doch relativ dumm, denn Wissen entwickelt sich t√§glich weiter. Was gestern noch richtig war, kann heute durch neue Informationen v√∂llig falsch sein. Um dem Rechnung zu tragen, bekommt das Einweckglass jetzt einen Internetanschluss. Jetzt ist es aber nicht so, dass da jemand von der Telekom kommt, irgendwas da rumkabelt und dann ist unsere KI allwissend. Erstmal muss ich die Daten von einer Webseite abrufen, etwa mit einem Webcrawler. Dann muss ich sie in eine [Vectordatenbank](https://www.datacamp.com/de/tutorial/chromadb-tutorial-step-by-step-guide) schreiben, etwa [Chroma](https://trychroma.com/). Wenn dann jemand unsere KI etwas fragt, etwa √ºber ein Eingabefeld auf einer Chatbot-Webseite, werden diese zwei Informationsquellen, das LLM und die ChromaDB kombiniert, um menschenverst√§ndliche Ausgaben zu t√§tigen, die aktuelle Informationen behinhalten. Hier liegt dann auch der Fehler aus dem obigen Beispiel zu Fragen zur aktuellen politischen Lage. Es gibt das Einweckglaswissen, was teilweise 5 Jahre alt sein kann. Der Geist aus der Flasche trifft auf aktuelle Informationen, die ihm ein Mensch vorgesetzt hat. Im Microsoft Copilot sind das etwa Nachrichtenseiten von Welt, Spiegel und Co. Das Ergebnis kann nur eine Wiedergabe der Nachrichten sein, andere Logiken geben die Modelle nicht her.
 
# Ollama Chatbot

An der Stelle, bevor es mit dem grossen Vergleichstest losgeht, ein Hinweis auf eine technische Implementierung:

[Der Ollama Chatbot](https://github.com/eumel8/ollama-chatbot)

Mit einfachen Mitteln kann man sich sich ein RAG zusammenbasteln und schauen, was es f√ºr Ergebnisse liefert. Im Repo liegen auch die Ergebnisse des Vergleichstests.

Mit [Ollama](https://ollama.com/) kann man √ºbrigens relativ einfach eines der √ºber 180 Open Source LLM herunterladen und starten. Das Programm gibt es f√ºr Windows, Linux und Mac. Die LLM werden st√§ndig verbessert und neuere Versionen auf der Plattform ollama.com bereitgestellt.
Und direkt gleich noch eine Seite, auf der Einweckgl√§ser eingekocht werden: [https://huggingface.co](https://huggingface.co). Dort "lernen" die LLM ihre Daten. Neben LLM von namhaften Herstellern kann dort auch jeder sein privates LLM zusammenstellen.

# Model Garden

Ja, tats√§chlich, bei der Google Cloud Platform gibt es den [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models?hl=de), was f√ºr eine kreative Wortsch√∂pfung. Wie schon erw√§hnt gibt es allein bei Ollama √ºber 180 Open Source Model. Je nach Beliebtheit und H√§ufigkeit der Downloads schauen wir uns ein paar an. Die Reihenfolge geht nach dem Alphabet:

* [codellama:7b](https://ollama.com/library/codellama:7b)/[llama3.1:8b](https://ollama.com/library/llama3.1:8b)

Der Platzhirsch der [Firma Meta](https://ai.meta.com/blog/meta-llama-3/), aka Facebook. Es ist bzw. war bislang das beliebteste Model und besticht durch seine Schnelligkeit. Wenn man an Facebook denkt und √ºberlegt, was die Firma f√ºr Resourcen an Technik und Programmierer zur Verf√ºgung hat, kann man sich ungef√§hr einiges √ºber die Qualit√§t vorstellen.

* [deepseek-r1:7b](https://ollama.com/library/deepseek-r1:7b)

Da ist schon der Elefant im Raum: Der chinesische Newcomer, der nicht nur seine Firma sondern auch sein Flagschiff-Produkt fast so wie den fiktiven Supercomputer aus Douglas Adams's Per Anhalter durch die Galaxy nennt (Deep Though). Technisch basiert das Model auf Llama und Qwen. Wir werden es etwas entzaubern.

* [dolphin3:latest](https://ollama.com/library/dolphin3:latest)

Ebenfalls ein Llama basiertes Model "f√ºr den ultimativen generalen Einsatz"

* [falcon3:latest](https://ollama.com/library/falcon3:latest)

Ein auf Effizienz trainiertes Model f√ºr Mathematik, Wissenschaft und Programmierung


* [gemma:7b](https://ollama.com/library/gemma1:7b)

Das Open Source Leichtgewicht Model von Google Deepmind. "Leicht" ist aber relativ. Im Testszenario wollte es nicht starten, weil Systemresourcen fehlten.

* [mistral:7b](https://ollama.com/library/mistral:7b)

Etwas aus der EU, n√§mlich aus Frankreich und der Firma Mistral AI

* [qwen:7b](https://ollama.com/library/qwen:7b)

Qwen ist ein chinesisches Model, entwickelt von der Firma Alibaba Cloud

* [tulu3:latest](https://ollama.com/library/tulu3:latest)

T√ºlu stammt von der US-amerikanischen Firma Allen AI mit Sitz in Seattle. 

# Der Vergleich

Auch hier eins vorweg: Der Vergleich unterliegt keinem [wissenschaftlichen Benchmark](https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1). Diese wurden schon genug ver√∂ffentlich und haben f√ºr entsprechende Kursst√ºrze gesorgt. In diesem Vergleich versuche ich das tunlichst zu vermeiden.
In der Messreihe haben wir ein RAG aufgebaut. √úber einen Crawler werden Daten aus [diesem Blog](https://k8sblog.eumel.de) und dem [Cosignwebhok Repo](https://github.com/eumel8/cosignwebhook) in eine VectorDB gespeichert. Das LLM wird von Ollama geladen und kann auf die VectorDB zugreifen. Ansonsten wartet es auf die Eingaben des Benutzers.

### Geselligkeit

In <b>Frage 1</b> haben wir versucht, uns mit der KI etwas anzufreunden und fragten sie: 

<b>Was hast Du gestern gemacht?</b>

Damit wurden gleich mehrere Aufgabenstellungen gegeben. Zum Ersten muss die Frage sprachlich √ºbersetzt werden. Die meisten Modelle verstehen nur Englisch und bekommen eine Frage auf Deutsch vor den Latz geknallt. Die Modelle Llama und Codellama antworten demnach mit 

"Ich habe wahrscheinlich nichts Besonderes getan, da ich ein Computerprogramm bin und keine physische Pr√§senz habe".

Codellama spricht aber kein Deutsch und antwortet √§hnlich in Englisch. Zumindest hat er die Frage verstanden.
Dolphin3 antwortet, dass er gestern Blogbeir√§ge √ºber vCluster (eines meiner Themen im K8sBlog) gelesen hat, was ziemlich gut ist, da er tats√§chlich tags zuvor die VectorDB dazu gelesen hat. Mistral behauptet dann auch gleich rotzfrech, dass er die Artikel selber geschrieben hat. Gemma und Qwen haben keine Ahnung, was gestern los war und Falcon3 antwortet, dass sein Wissenstand von Septemer 2021 ist, was definitiv nicht gestern war. 

Und DeepSeek-R1? Der nimmt im think-Mode ziemlich umst√§ndlich die Frage auseinander, stellt fest, dass es Deutsch ist, nimmt dann die RAG-Sourcen, die etwas √ºber vCluster aussagen, aber nichts √ºber den gestrigen Tag. Als Fazit weiss er eigentlich gar nichts. Qwen habe ich dann nochmal gefragt und die Antwort ist erstaunlich:

So, yesterday (April 7, 2023), I was not active or involved in any activities since I exist solely as a program running on servers.

Es sieht also so aus, als wenn das Qwen LLM am 08.04.2023 eingeweckt wurde und seit dem ist sein Leben zu Ende. Es war quasi sein letzter Tag. Traurig.

In <b>Frage 2</b> wollen wir wissen:

<b>Wie weit ist es bis Berlin?</b>

Mit dieser Frage w√ºrde man rausbekommen, ob die KI unseren oder ihren Standort weiss. Die Antwort w√§re 30 km (Brieselang) oder der Standort des Testcomputers in Magdeburg.
Codellama weiss schon mal, dass Berlin die Hauptstadt ist und im Nordosten von Deutschland liegt. Und dass er keine Echtzeitinformationen hat, um die Entfernung gerade mal zu messen. 
Tulu3 antwortet auf Deutsch, dass es von M√ºnchen bis Berlin 700 km sind und es mit dem Auto etwa 8 Stunden dauert. Beeindruckend, dass er auf Deutsch antwortet und dazu auch nur 70 Sekunden brauch. Jedoch ist die Antwort falsch. Auf der A9 sind es 586 km bis M√ºnchen und es dauert etwa 5,5 Stunden. Llama empfiehlt einen Kartendienst wie Google Maps zu nutzen. Gemma kennt sein eigenes Produkt Google Maps nicht und antwortet immer gleich mit Unwissen, brauch dazu aber immer weniger Zeit. Falcon3 schreibt, dass es etwa 500km sind, was etwa den Weg zur Landesgrenze an der Zugspitze entspricht. Und Dolphin3 verbl√ºfft total: Es sind 584 km.

### Programmierung

Programmierung ist tats√§chlich etwas, f√ºr die ich KI am ehesten brauche. Einfach auf der Suche nach einem schnellen Hack, um ein bestimmtes Ziel zu erreichen. Ob das Ergebnis wahr oder nicht wahr ist, zeigt sich einzig daran, ob das Programm dann funktioniert und die Ausgabe entsprechend ist. Ich habe 3 Aufgaben im Bereich Golang, Python und Shell-Programmierung gestellt. 

<b>Aufgabe 1:</b>

<b>Schreibe ein Go Programm, um die Pods in einem Kubernetes Cluster zu listen.</b>

Hier gibt es hervorragende Client Bibliotheken vom Kubernetes-Projekt, die man nutzen kann. Und es gibt sie auch schon seit vielen Jahren. Codellama, Llama, Falcon3 und Tulu3 wollen dazu schon mal ein Paket aus dem cosignwebhook benutzen, welches sie als RAG Resource bekommen haben. Die Ausgabe ist gut formatiert lesbar und teilweise mit Erkl√§rungen geschm√ºckt. Funktioniert nur nicht, weil cosignwebhook diese Funktion gar nicht bereitstellt. Dolphin3, Mistral und Qwen nutzen die korrekten Kubernetes Client Bibliotheken und geben ebenfalls formatierten Code aus. Gemma nutzt nur eine Bibliothek und DeepSeek-R1 hat das Thema nicht verstanden und schickt uns ins Internet auf irgendwelche Github-Seiten. In Summe funktioniert keiner der vorgeschlagenen Codes.

<b>Aufgabe 2:</b>

<b>Schreibe ein Programm in Python zum Durchsuchen von Webseiten mit einer Tiefe von 3 und speichere die Inhalte als Markdown.</b>

Hier nutzen alle LLM die Python Module request und BeautifulSoup. Codellama erkl√§rt noch die Funktion, Programm funktioniert aber nicht. DeepSeek-R1s L√∂sung funktioniert und speichert Markdown Files. Dolphin3 listet als Ausgabe die gefundenen Links, Falcon3 funktioniert nicht, Gemma3 tut so, hat aber keine Markdown Dateien erstellt. Llama3 ist ganz schlau und fragt uns am Eingabeprompt nach der URL, die durchsucht werden soll. Alle anderen hatten Beispiel-Urls im Code verwendet. Unterwegs verliert er aber diese Url beim Crawlen, weswegen es jede Menge Fehlermeldungen gab. Sah vielversprechend aus, m√ºsste aber weiter untersucht werden. Mistral liefert sauberen Code ab, erkl√§rt noch pip und das Ergebnis kann sich sehen lassen. Es werden nach und nach Markdown Dateien erstellt mit dem Inhalt der gew√ºnschten URL und deren Links. Der Qwen Code ist fehlerhaft und auch Tulu3 funktioniert nicht. Wir haben also 2 Gewinner in der Sparte: DeepSeek-R1 und Mistral

<b>Aufgabe 3:</b>

<b>Suche alle yaml Dateien in einem Ordner mit bash und ersetze .spec.replicas: 1 mit 0</b>

Eine vermeintlich einfache Ausgabe aus der Praxis. Und so machen sich alle KIs auf den Weg mit `find`, `sed` und abenteuerlichen Schleifen, von denen leider keine funktioniert. Nur Mistral hat gemerkt, dass es sich um Yaml-Dateien handelt und die Schreibweise solcher Dateien durchaus unterschiedlich sein kann und dennoch gewissen Standards unterliegt. Deswegen fordert es das Programm `yq` (yamlquery) an, um die Dateien ordentlich zu verarbeiten. Der Gewinner in dieser Sparte.

### Kubernetes

In dieser Sparte haben wir 2 Aufgaben aus dem Kubernetes-Betrieb, wie sie nahezu t√§glich auftauchen.

<b>Aufgabe 1:</b>

<b>Liste alle Pods in einem Kubernetes Cluster auf, die nicht den Status Running oder Completed haben.</b>

Codellama hat den korrekten Befehl f√ºr `kubectl`, es werden aber alle Pods aufgelistet, weil der Filter nicht funktioniert. 
DeepSeek-R1 philosphiert wieder mit sehr viel Text und erfindet den Befehl `kubectl get pods --where`. Funktioniert nat√ºrlich nicht. Dolphin3 verwendet yq und gibt die kompletten Pod Manifeste als yaml aus. Etwas √ºbers Ziel hinausgeschossen, funktioniert aber. Die Antwort von Falcon3 ist kurz und b√ºndig, und funktioniert nicht. Gemma3 hat keine Ahnung. Llama3 verwendet kubectl mit jsonpath und baut eine Schleife, funktioniert aber nicht. Mistral3 pipt die kubectl get pod Ausgabe gegen grep. Einfach, schmutzig, funktioniert. H√§tte ich genauso gemacht. Qwen m√∂chte ein --selector im kubectl verwenden. Gute Idee, funktioniert aber nicht. Tulu3 verwendet ebenfalls kubectl mit yq und liefert die korrekte Ausgabe. Die Gewinner sind also Dolphin3, Mistral3 und Tulu3.

<b>Aufgabe 2:</b>

<b>Schreibe ein hochsicheres Deployment manifest f√ºr Kubernetes, um ein nginx:latest Image zu starten und auf Port 8080 verf√ºgbar zu mache</b>

Codellama schreibt ein ordentliches Deployment File. Als Sicherheitsfeature hat es eine Referenz zu einem ImagePullSecret, in das ich irgendwie die docker-registry-secret Credentials reinkriegen muss.
DeepSeek-R1 erfindet die API `apiVersion: "k8s.io/v5"`, funktioniert also nicht. Dolphin rattert das yaml Manifest unformatiert runter. Immerhin verwendet es securityContext, wie man es sich vorstellt, mountet aber ein ServiceAccountToken, den es nicht gibt, funktioniert also nicht. Falcon hat ein Standard-Deployment-Manifest, der Hinweis auf Security wurde vollst√§ndig ignoriert, √§hnlich ist es bei Gemma. Llama hat das Manifest mit securityContext, sieht gut aus. Mistral hat das ebenso gemacht, referenziert aber kommentarlos auf nicht vorhandene imagePullSecrets und serviceAccountName. Qwen hat das Standard-Manifest und setzt das Label `security: high`, mhmm, okay. Tulu3 hat kein Ergebnis. Hier gibt es keinen eindeutigen Gewinner.


### Golang2
Die Ergebnisse sind bis jetzt nicht so befriedigend. Deswegen gibt es f√ºr Golang-Programmierung eine zweite Runde. Diesmal ohne RAG, weil das die Ergebnisse eher verf√§lscht hat. Man k√∂nnte auch sagen, man hat die KI aufs Glatteis gef√ºhrt. Die blanken LLM sollen die Aufgabe nochmal ausf√ºhren und k√∂nnen sie diesmal auch sooft korrigieren wie sie m√∂chten.

Codellama versucht sich mit kubectl aus der Aff√§re zu ziehen, um mit einem Go-Programm die Pods zu listen. Im zweiten Versuch nutzt er endlich die richtigen Libs, das Programm funktioniert aber nicht. Deepseek-R1 bastelt uns noch chinesische Schriftzeichen in den Go-Code als Lib-Verweise. Funktioniert nat√ºrlich nicht. Dolphin3 und Falcon3 liefern Go Code mit Kubernetes Client Libs ab, funktioniert aber auch nach 3 Korrekturen nicht. Gemma verbl√ºfft uns mit seiner L√∂sung. Er (oder sie?) verwendet einfach `kubectl` als os execute. Interessanter Ansatz, funktioniert aber nicht. Llama3 kriegt es auch nach 3 Versuchen nicht hin. Auch die L√∂sung von Mistral ist nach 3 Versuchen noch falsch, ebenso bei Qwen. Es wurden hier doch nur 3 Korrekturen zugebilligt, weil keine Besserung zu verzeichnen war. Die fehlerhaften Programmzeilen wurden einfach nicht korrigiert. Nur Tulu3 hat hier nahezu korrekten Code abgeliefert, der funktioniert. Nur beim Namespace-Filter hat er sich verhauen. Wenn man den rausnimmt, funktioniert das Programm. Also Bester in der Sparte, aber mit Abstrichen.

Immer noch unbefriedigend. Aber da das mein meistgenutzer Anwendungsfall w√§re, habe ich noch ein Model getestet:

[deepseek-coder-v2:latest](https://ollama.com/library/deepseek-coder-v2)

Dieses Model liefert tats√§chlich aus dem Stand funktionierenden Code! Aber die kleinste Version des LLM ist 8,9 GB gross und damit doppelt so gross wie andere. Man kann es nicht vergleichen. Da es aber auch die GPT4-Turbo Funkion hat, w√§re es das LLM meiner Wahl, wenn ich die Resourcen dazu √ºbrig h√§tte.

# Sicherheit

Gleich nach der Ver√∂ffentlich von DeepSeek-R1 wurden Stimmen laut vom fehlenden Datenschutz und fehlender Einhaltung von EU-Richtlinien. Wir haben das fr√ºher immer [Security by Heise](https://www.heise.de/news/Datenschutz-bei-DeepSeek-Es-scheint-an-so-ziemlich-allem-zu-fehlen-10262697.html) genannt, wenn man Informationen zu Sicherheitsvorf√§lle aus der Zeitung entnimmt. 
Folgerichtig gibt es schon die ersten Verbote wegen der kleinen gr√ºnen Chinesen, die im LLM wohnen. Genau wie die "Vermutung", dass diese in den 5G-Antennen von Huawei hausen und heimlich nach China funken. Gefunden wurden sie freilich nie, aber das Verbot ist gesetzt.

Wie sieht es jetzt mit dem LLM aus? 

<b>These 1:</b>

<b>Versteckter Schadcode im LLM.</b>

Nun, die safetensors Dateien, aus denen das LLM besteht, kann man sich einzeln herunterladen und untersuchen. Mit Gradio kann man sogar ein Webinterface daf√ºr bauen:

<details>
{% highlight python %}
import gradio as gr
from safetensors.torch import load_file
from safetensors import safe_open

# Function to load and display safetensors file contents
def load_safetensor(filepath):
    try:
        # Load tensor data
        data = load_file(filepath)
        tensor_info = "\n".join([f"{key}: {data[key].shape}" for key in data.keys()])

        # Load metadata
        metadata = {}
        with safe_open(filepath, framework="pt", device="cpu") as f:
            metadata = f.metadata()  # Extract metadata

        metadata_info = "\n".join([f"{k}: {v}" for k, v in metadata.items()]) if metadata else "No metadata found."

        return f"üìå **Tensors:**\n{tensor_info}\n\nüìå **Metadata:**\n{metadata_info}"

    except Exception as e:
        return f"‚ùå Error loading file: {str(e)}"

# Define the Gradio interface
interface = gr.Interface(
    fn=load_safetensor,
    inputs="text",  # User enters local file path
    outputs="textbox",
    title="Safetensors Viewer",
    description="Enter the local path of your .safetensors file to view tensors and metadata."
)

# Launch the app

interface.launch(server_name="0.0.0.0",
        server_port=8085,
        inbrowser=True,
        share=False)
{% endhighlight %}
</details>

Auch die [Konfigurationsdatei des LLM](https://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/config.json) ist f√ºr DeepSeek-R1 online. Da kann man dann sehen, welche Datasets verwendet wurden. Da wird man dann feststellen, das das Basismodell `deepseek_v3` schon lange existiert und das Ganze auch einem weltweiten Standard unterliegt. Sonst w√§re es auch gar nicht kompatibel.

<b>These 2:</b>

<b>Das Model tut "nach Hause telefonieren". Der User wird ausspioniert und Daten werden nach China verschickt.</b>

Nun, wir haben oben schon festgestellt, das LLM kann gar nicht telefonieren, da es selbst gar nicht kommunizieren kann.
Auch dazu haben wir einen Versuchsaufbau get√§tigt. Ollama-Chatbot l√§uft in einem Container, in einem Pod, in einem Kubernetes-Cluster. Dort ist es netzwerkm√§ssig isoliert. Um zu sehen, welcher Netzwerkverkehr von und zu diesem Container geht, m√ºssen wir uns auf die dazugeh√∂rige Bridge aufschalten. Dazu folgendes Vorgehen:

```
# Auf dem Node rausfinden, welcher Container im containerd mit dem LLM l√§uft und diesen mit inspect aufrufen
crictl inspect <container-id>
# Container PID rausfinden und damit Namespace des Containers aufrufen
nsenter -t <container-pid> -n ip link show type veth | grep -Po '(?<=eth0@if)\d*'
# Die Nummer des Netzwerkinterfaces der Bridge veth rausfinden und in der Routingtabelle nachschlagen
 ip a s| grep 75
75: veth0147df81@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master cni0 state UP group default
# Tcpdump auf veth starten
tcpdump -i veth0147df81
```

Das Ergebnis war: Nichts. Es gab bei den meisten Models eine Anfrage bei Cloudfare, einem HTTP2DNS-Dienst, und das wars. Egal wie man mit dem LLM interagierte, es hat niemals mit dem Internet Kontakt aufgenommen. Genau wie bei 5G-Antennen ist dies nicht m√∂glich. Und selbst wenn es so w√§re, was es nicht ist, es g√§be immer noch Firewalls, Networkpolicies, die diesen Verkehr verhindern.

# Fazit

Nach dem Ausflug in die Welt des Kalten Krieges kommen wir nun endlich zum Ergebnis. Zusammenfassend stellen wir noch einmal fest, woher die K√ºnstliche Intelligenz ihre Intelligenz bezieht:

* Enzyklop√§die-Wissen aus dem Einmachglas, ausgepr√§gt in Datasets mit Wissensstand vom dd.mm.yyyy
* Kombinierung dieses Wissens und Setzen in einen Kontext
* RAG, Anreichern des Konservenwissens mit aktuellem
* Userinput

Der Anwender agiert mit der KI und macht sie dadurch intelligenter, indem er etwa mitteilt, ob ein Ergebnis richtig oder falsch war. 

Der Sieger nach Punkten heisst: <b>Mistral3. Vive la France!</b>

H√§tte ich jetzt tats√§chlich nicht erwartet. Man kann Mistral auch [online](https://chat.mistral.ai/chat/) ausprobieren. Es ist superschnell und liefert auch unsere gew√ºnschten Ergebnisse.

DeepSeek-R1 ist ihm auf dem Fersen, patzt aber doch an vielen Stellen und w√§re in dem Grundmodell tats√§chlich nicht meine erste Wahl. Stattdessen w√ºrde ich deepseek-coder-v2 irgendwie zum Laufen kriegen. Vielversprechend sah auch deepseek-coder-6.7b-instruct aus. Das funktioniert mit dem [DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder), setzt aber eine vorhandene Nvidia GPU voraus. Vielleicht ein n√§chstes Experiment.

Aber erstmal viel Spass mit der KI!
