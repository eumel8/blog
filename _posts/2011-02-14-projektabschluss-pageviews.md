---
layout: post
tag: general
title: Projektabschluss Pageviews
subtitle: "So ist es recht. Kurz vorm naechsten Urlaub konnte dieses Projekt abgeschlossen werden. Inspiriert  durch eine grosse Anzeigetafel in der Firmenzentrale eines Mobilfunkanbieters in England und auf der Suche nach sinnvollen Anwendungen fuer Hadoop und nosql"
date: 2011-02-14
author: eumel8
---

<p>So ist es recht. Kurz vorm naechsten Urlaub konnte dieses Projekt abgeschlossen werden. Inspiriert  durch eine grosse Anzeigetafel in der Firmenzentrale eines Mobilfunkanbieters in England und auf der Suche nach sinnvollen Anwendungen fuer Hadoop, nosql a ka  Hbase und REST stellte sich mir die Aufgabe, die Pageviews einer Webseite darzustellen - in Echtzeit.</p>
<br/>
<p>Ueblicherweise werden Zugriffe auf Webseiten von der verwendeten Applikation wie Apache Httpd mitgeloggt. Es entstehen Logfiles mit systematischem Aufbau wie Uhrzeit, IP-Adresse, aufgerufene Seite, uebertragene Daten, Browserversion usw. Das Sammeln dieser Daten ist zwar umstritten, aber nicht Thema dieser Abhandlung.  Die Auswertung der Daten erfolgt mit Tools wie Webalizer oder Awstats einmal im Monat oder maximal taeglich. Von den vielen Ergebnissen, die so eine Auswertung bietet, interessiert uns nur ein Wert. Die Pageviews, also die Anzahl der angerufenen Webseiten ist <strong>der </strong>Index fuer Webmarketing und Ranking (www.alexa.com).</p>
<p>Jetzt haette man diese Zahlen auch durch Google-Analytics (oder Omniture etc.) berechnen koennen oder auf jeder Webseite ein Blind-Gif einbauen, um diese dann etwa als CGI irgendwo zu zaehlen, aber da muss man ja jede Webseite anfassen (auch die dynamisch generierten) und ist erstmal ewig beschaeftigt und hat vor allem keine sinnvolle Anwendung fuer Hadoop ;-)</p>
<p>Dessen Zusammenspiel habe ich weiter hinten schon beschrieben. Fuer dieses Projekt brauchen wir</p>
<ul>
<li>Einen funktionierenden Hadoop-Cluster mit Datanodes und Map&amp;Reduce-Nodes</li>
<li>Hbase</li>
<li>Stargate fuer Hbase</li>
<li>pig (mit der library piggybank)</li>
<li>Einen Webserver mit PHP und Curl</li>
</ul>
<p>Los gehts:</p>
<p>Die Logfiles liegen ueblicherweise gezipt auf den Applikationsservern oder werden auf einen Fileserver archiviert. Es gibt zwar Ansaetze mit Log2MySQL, die Daten gleich in eine relationale Datenbank zu schreiben, aber zum einen ist das nicht vhost-faehig und zum anderen  performed das nicht. Wenn die Datenbank haengt, handelt man sich wahrscheinlich sogar ein Bottleneg auf dem Webserver ein.</p>
<p>Schauen wir uns zuerst das Herzstueck unserer Web 2.0 Anwendung an - das pig-Skript</p>
<pre># vi /usr/local/pig/apacheAccessLogAnalysis_pageview.pig <br />-- Registriere piggybank fuer Logformat-Schema<br />register /usr/local/pig-0.8.0/contrib/piggybank/java/piggybank.jar;<br /><br />DEFINE LogLoader org.apache.pig.piggybank.storage.apachelog.CombinedLogLoader();<br /><br />-- Lade Logs ins LogLoader Schema<br /><br />logs = LOAD '$LOGS' USING LogLoader<br /> as (remoteAddr, remoteLogname, user, time, method, uri, proto, status, bytes, <br /> referer, userAgent);<br /><br />-- Filter Logrequests nach Methoden, Groesse, Pageviews und Statuscodes<br />logs = FILTER logs BY method == 'GET'<br /> AND bytes != '-' <br /> AND (NOT (uri matches '.*(css|js|gif|png|jpg|bmp|ico|swf|jpeg|class)$'))<br /> AND status &gt;= 200 AND status &lt; 300;<br /><br />logsfull = FILTER logs BY method == 'GET'<br /> AND bytes != '-'<br /> AND status &gt;= 200 AND status &lt; 300;<br /><br />-- Generiere Report fuer uri, status und bytes<br />logs = FOREACH logs GENERATE uri, status, bytes;<br /><br />groupedByCount = GROUP logs ALL;<br /><br />-- Summe aller PageViews bestimmen<br />sumCounts = FOREACH groupedByCount GENERATE COUNT(logs.$0);<br /><br />-- Ausgabe aller Werte ins HDFS<br />STORE sumCounts INTO 'sum_counts';<br /></pre>
<p>piggybank brauchen wir, weil es schon vordefinierte Parser fuer Apache-Logfiles hat. Wir muessen nur drauf achten, dass unser Logformat auch stimmt. Dann filtern wir unsere Zugriffe nach Gueltigkeit (Statuscode), Groesse (mindestens &gt; 0 byte) und keine Bilder oder andere eingebettete Objecte. Die letzte Anweisung gleicht dem select count(*) einer Datenbankanweisung.</p>
<p>Unser Hbase liegt natuerlich im selben Hadoop-Cluster, aber ich habe keine Moeglichkeit gefunden, die Daten direkt aus Hadoop fuers Web auszulesen. Eventuell wuerde das im zunehmenden Masse unuebersichtlich werden. Also muessen die  Ergebnisse aus Pig von Hadoop ins Hbase. Da wir spaeter REST verwenden, schreiben wir die Daten auch mit REST ins Hbase. Dazu brauchen wir aber einen REST-Server. Den bekommen wir mit Stargate. Das ist einfach eine Java-Klasse, die wir aus unserem Hbase heraus starten, womit sofort eine Webschnittstelle fuer unsere nosql Datenbank zur Verfuegung steht:</p>
<blockquote>
<pre>cd /usr/local/hbase/; bin/hbase org.apache.hadoop.hbase.stargate.Main -p 60050<br /></pre>
</blockquote>
<p>Unser REST-Server laucht auf Port 60050. Und wir brauchen ein Schema:</p>
<blockquote>
<pre>cd /usr/local/hbase/; bin/hbase shell<br />HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.<br />Version: 0.20.4, r941076, Tue May 4 15:23:44 PDT 2010<br />hbase(main):001:0&gt; create "pageviews", "web"</pre>
</blockquote>
<p>Auf http://hdp-cl01.eumel.de:60050/ sollte "pageviews" stehen. So funktioniert unser REST-Server.</p>
<p>Da wir spaeter PHP als Skriptsprache einsetzen, habe ich mich zum Sammeln der Daten und Starten des Pig auch fuer PHP von der Kommandozeile entschieden:</p>
<pre># vi /usr/local/scripts/pagewrite.php<br />&lt;?php<br /><br />// Erstellung Pageview Statistik <br /><br />putenv("JAVA_HOME=/usr/java/jre1.6.0_20/");<br /><br />$pageview= 0;<br />$hadoop="/usr/local/hadoop/bin/hadoop";<br />$pig="/usr/local/pig/bin/pig";<br />$tod = date("Ymd");<br /><br /><br />exec("$hadoop dfs -copyFromLocal /logs/www01/combined_log.".$tod."*.gz /weblogs/www01/");<br />exec("$hadoop dfs -copyFromLocal /logs/www02/combined_log.".$tod."*.gz /weblogs/www02/");<br />exec("$hadoop dfs -copyFromLocal /logs/www03/combined_log.".$tod."*.gz /weblogs/www03/");<br />exec("$hadoop dfs -copyFromLocal /logs/www04/combined_log.".$tod."*.gz /weblogs/www04/");<br />exec("$hadoop dfs -copyFromLocal /logs/www05/combined_log.".$tod."*.gz /weblogs/www05/");<br />exec("$hadoop dfs -copyFromLocal /logs/www06/combined_log.".$tod."*.gz /weblogs/www06/");<br />exec("$hadoop dfs -copyFromLocal /logs/www07/combined_log.".$tod."*.gz /weblogs/www07/");<br />exec("$hadoop dfs -copyFromLocal /logs/www08/combined_log.".$tod."*.gz /weblogs/www08/");<br />exec("$hadoop dfs -rmr /user/root/sum_counts");<br />exec("$pig -x mapreduce -f /usr/local/pig/apacheAccessLogAnalysis_pageview.pig \<br /> -param LOGS='/weblogs/www*/combined_log.".$tod."*.gz'");<br /><br />exec("$hadoop dfs -cat /user/root/sum_counts/part*", $output);<br /><br />while(list(,$line) = each($output)) {<br /> $pageview=$pageview + $line;<br />}<br /><br />$row = base64_encode('www.eumel.de');<br />$col = base64_encode('web:'.$tod);<br />$val = base64_encode($pageview);<br />$time = time();<br /><br /><br />$xml = '<br />&lt;CellSet&gt;<br />&lt;Row key="'.$row.'"&gt;<br />&lt;Cell timestamp="'.$time.'" column="'.$col.'"&gt;'.$val.'&lt;/Cell&gt;<br />&lt;/Row&gt;<br />&lt;/CellSet&gt;<br />';<br /><br />$session = curl_init("http://hdp-cl01.eumel.de:60050/pageviews/www.arcor.de");<br /><br />curl_setopt($session, CURLOPT_POST, 0);<br />curl_setopt($session, CURLOPT_VERBOSE, 0);<br />curl_setopt($session, CURLOPT_HEADER, 0);<br />curl_setopt($session, CURLOPT_POSTFIELDS, $xml);<br />curl_setopt($session, CURLOPT_HTTPHEADER, array("Content-Type: text/xml;charset=UTF-8"));<br />curl_setopt($session, CURLOPT_RETURNTRANSFER, true);<br /><br />$response = curl_exec($session);<br />curl_close($session);<br /><br />echo $response;<br /><br /></pre>
<p>Unser PHP-Shell-Skript fungiert also als REST-Client. Es gibt zwar unzaehlige Implementierungsversuche (am liebsten auch in CodeIgniter), aber zum Schluss verwenden die meisten dann doch Curl. Dort kann man einfach HTTP-Header manipulieren. Am schwierigsten war, das Script davon zu ueberzeugen, doch bitteschoen text/xml zum Server zu senden und nicht text/html, wie es es default tut. Das XML selbst ist minimal mit einem Wert (Value) fuer eine Zeile (Column) in einer Spalte (Row). Das Abstrakte hierbei ist das Dreidimensionale, mit dem ich immer mal wieder Probleme habe: Jeder Wert in jeder Zeile und Zelle hat eine Versionierung, die sich ueber den Zeitstempel definiert. Wenn ich diesen mit abfrage, habe ich beliebig viele Werte fuer diese eine Zelle in der Tabelle. Ansonsten wird immer der neueste Wert ausgegeben.</p>
<p>Fragen wir doch unsere Pageviews ab, nachdem wir das pagewrite.php zyklisch per Cron aufrufen lassen.</p>
<pre># vi pageview.php<br />&lt;?php<br /><br />$tod = date("Ymd");<br /><br />if(isset($_GET['useday'])) {<br /> $useday = $_GET['useday'];<br /> } else {<br /> $useday = $tod;<br /> }<br /><br />if(isset($_GET['hour'])) {<br /> $hour = $_GET['hour'];<br /> } else {<br /> $hour = "0";<br /> }<br /><br />if ((!ereg("^[0-9]+$", $useday) || (strlen($useday) &gt; 8) || (strlen($useday) &gt; 8))){<br />echo "Falsche Eingabe"; exit;}<br /><br />if ((!ereg("^[0-9]+$", $hour) || (strlen($hour) &gt; 2) )){<br />echo "Falsche Eingabe"; exit;}<br /><br />$xmlUrl = "http://hdp-cl01.eumel.de:60050/pageviews/www.arcor.de/web:".$useday;<br /><br />if(!isset($xmlUrl)) exit;<br />$xmlStr = @file_get_contents($xmlUrl);<br /><br />if ($xmlStr === false)<br />{<br /> exit;<br />} else {<br /><br />$xmlObj = simplexml_load_string($xmlStr);<br />$arrXml = objectsIntoArray($xmlObj);<br />// print_r ($arrXml);<br /> $res = base64_decode($arrXml['Row']['Cell'][$hour]);<br /> if( $res == 0) {<br /> $res = base64_decode($arrXml['Row']['Cell']);<br /> }<br />echo $res;<br />}<br /><br />function objectsIntoArray($arrObjData, $arrSkipIndices = array())<br />{<br /> $arrData = array();<br /><br /> // if input is object, convert into array<br /> if (is_object($arrObjData)) {<br /> $arrObjData = get_object_vars($arrObjData);<br /> }<br /><br /> if (is_array($arrObjData)) {<br /> foreach ($arrObjData as $index =&gt; $value) {<br /> if (is_object($value) || is_array($value)) {<br /> $value = objectsIntoArray($value, $arrSkipIndices); // recursive call<br /> }<br /> if (in_array($index, $arrSkipIndices)) {<br /> continue;<br /> }<br /> $arrData[$index] = $value;<br /> }<br /> }<br /> return $arrData;<br />}<br /><br />?&gt;<br /><br /><br /></pre>
<p>Die Funktion ist einfach dem php-Manual vom xmlparser uebernommen. Es schreibt im Prinzip die XML-Werte in ein Array. Unser PHP-Script vertraegt 2 Eingabevariablen</p>
<ul>
<li>useday - Datum der Pageview-Statistik im Format "20110214"</li>
<li>hour - Stunde (Version) der Pageview-Statistik (0 = neueste ... 24 = aelteste)</li>
</ul>
<p>Die Variablen werden auf Eingabemaengel geprueft (sollte man sowieso immer tun) und als Ausgabe erhalte ich einfach die Pageviews als Zahl. Diese kann dann in mein <a href="http://www.directindustry.de/prod/strongarm-designs/grossformat-lcd-bildschirme-15943-364599.html" target="_blank">Praesentationslayer</a> eingebunden werden.</p>
<p> </p>
<p>[1] http://wiki.apache.org/pig/PiggyBank</p>
<p>[2] http://wiki.apache.org/hadoop/Hbase/Stargate<br />[3] http://php.net/manual/de/book.curl.php<br />[4] http://pig.apache.org<br />[5] http://php.net/manual/de/book.simplexml.php<br />[6] http://hbase.apache.org</p>
