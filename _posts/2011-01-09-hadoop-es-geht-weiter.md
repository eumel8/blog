---
layout: post
tag: inet
title: Hadoop - es geht weiter
subtitle: "Ich komme davon nicht los. Hadoop, HDFS, Hbase und der ganze REST. Es ist wie ein Labyrinth oder Puzzle, bei dem man nicht weiss, ob es sich als Ganzes aufloest, wenn man weiter herantritt oder weiter wegstehen muss. Was bislang geschah: Wir haben 4 Rechner..."
date: 2011-01-09
author: eumel8
---

<p>Ich komme davon nicht los. Hadoop, HDFS, Hbase und der ganze REST. Es ist wie ein Labyrinth oder Puzzle, bei dem<br />man nicht weiss, ob es sich als Ganzes aufloest, wenn man weiter herantritt oder weiter wegstehen muss.<br />
<br/>
Was bislang geschah:<br />Wir haben 4 Rechner mit SLES-10 Betriebssystem 32bit (oder besser 64bit, wenn wir mehr RAM-Speicher adressieren<br />wollen)<br />Wir legen auf allen 4 Rechnern den Benutzer "hadoop" an und praeperieren den ssh-key fuer den Benutzer:</p>
<blockquote># ssh-keygen -q -t rsa -N ''<br /># cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys<br /># scp -r ~/ssh hdp-cl02:~/<br /># scp -r ~/ssh hdp-cl03:~/<br /># scp -r ~/ssh hdp-cl04:~/<br /></blockquote>
<p>und installieren Hadoop  nach /home/hadoop auf alle Rechner:</p>
<blockquote># wget http://www.apache.org/dist/hadoop/core/hadoop-0.20.2/hadoop-0.20.2.tar.gz<br /># tar xvfz hadoop-0.20.2.tar.gz<br /># ln -s hadoop-0.20.2 hadoop<br /># scp -r hadoop* hdp-cl02:~/<br /># scp -r hadoop* hdp-cl03:~/<br /># scp -r hadoop* hdp-cl04:~/<br /></blockquote>
<p><br />Es erfolgt die Konfiguration unseres Hadoop- und HDFS-Clusters, denn HDFS ist standardmaessig in Hadoop enthalten:<br />/home/hadoop/hadoop/conf/masters</p>
<pre>hdp-cl01</pre>
<p> </p>
<p>/home/hadoop/hadoop/conf/slaves</p>
<p> </p>
<pre>hdp-cl01<br />hdp-cl02<br />hdp-cl03<br />hdp-cl04</pre>
<p> </p>
<p>/home/hadoop/hadoop/conf/hdfs-site.xml</p>
<pre>&lt;?xml version="1.0"?&gt;<br />&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br />&lt;configuration&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.replication&lt;/name&gt;<br />&lt;value&gt;2&lt;/value&gt;<br />&lt;description&gt;Default block replication.<br />The actual number of replications can be specified when the file is created.<br />The default is used if replication is not specified in create time.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.name.dir&lt;/name&gt;<br />&lt;value&gt;/tmp/hadoop-root&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;dfs.data.dir&lt;/name&gt;<br />&lt;value&gt;/tmp/hadoop-data&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;/configuration&gt;<br />/home/hadoop/hadoop/conf/mapred-site.xml<br />&lt;?xml version="1.0"?&gt;<br />&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br />&lt;configuration&gt;<br />&lt;property&gt;<br />&lt;name&gt;mapred.job.tracker&lt;/name&gt;<br />&lt;value&gt;hdp-cl01:54311&lt;/value&gt;<br />&lt;description&gt;The host and port that the MapReduce job tracker runs<br />at. If "local", then jobs are run in-process as a single map<br />and reduce task.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;mapred.system.dir&lt;/name&gt;<br />&lt;value&gt;/tmp/shadoop&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;mapred.local.dir&lt;/name&gt;<br />&lt;value&gt;/tmp&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;mapred.map.tasks&lt;/name&gt;<br />&lt;value&gt;40&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;mapred.reduce.tasks&lt;/name&gt;<br />&lt;value&gt;8&lt;/value&gt;<br />&lt;/property&gt;<br />&lt;/configuration&gt;<br />/home/hadoop/hadoop/conf/core-site.xml<br />&lt;?xml version="1.0"?&gt;<br />&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br />&lt;configuration&gt;<br />&lt;property&gt;<br />&lt;name&gt;fs.default.name&lt;/name&gt;<br />&lt;value&gt;hdfs://hdp-cl01:54310&lt;/value&gt;<br />&lt;description&gt;The name of the default file system. A URI whose<br />scheme and authority determine the FileSystem implementation. The<br />uri's scheme determines the config property (fs.SCHEME.impl) naming<br />the FileSystem implementation class. The uri's authority is used to<br />determine the host, port, etc. for a filesystem.&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;/configuration&gt;<br /><br /></pre>
<p>Die Konfiguration wird ebenfalls auf alle Rechner verteilt, obwohl das nicht unbedingt notwendig ist:</p>
<p> </p>
<blockquote>for i in 02 03 04 ; do scp /home/hadoop/hadoop/conf/* hdp-cl$i:~/hadoop/conf;done<br /></blockquote>
<p> </p>
<p>Mit</p>
<blockquote>
<p>/home/hadoop/hadoop/bin/start-all.sh</p>
</blockquote>
<p>starten wir den Masternode hdp-cl01, die Mapreduce-Nodes hdp-cl01 - hdpcl04<br />und die DFS-Nodes hdp-cl01 - hdp-cl04.<br />Mit</p>
<blockquote>
<p>/home/hadoop/hadoop/bin/hadoop dfsadmin -report</p>
</blockquote>
<p>koennen wir uns den Status von HDFS ansehen.<br />Wir besorgen uns Hbase und installieren es auf alle 4 Rechner:</p>
<blockquote>
<p># cd ~<br /># wget http://www.apache.org/dist/hadoop/hbase/hbase-0.20.5/hbase-0.20.5.tar.gz<br /># tar xvfz hbase-0.20.5.tar.gz<br /># ln -s hbase-0.20.5 hbase</p>
</blockquote>
<p><br />Es erfolgt die Konfiguration unseres HBase-Clusters:</p>
<p>/home/hadoop/hbase/conf/regionservers</p>
<pre>hdp-cl01<br />hdp-cl02<br />hdp-cl03<br />hdp-cl04<br /></pre>
<p>/home/hadoop/hbase/conf/hbase-site.xml</p>
<pre>&lt;?xml version="1.0"?&gt;<br />&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br />&lt;configuration&gt;<br />&lt;property&gt;<br />&lt;name&gt;hbase.rootdir&lt;/name&gt;<br />&lt;value&gt;hdfs://hdp-cl01:54310/hbase&lt;/value&gt;<br />&lt;description&gt;The directory shared by region servers.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;<br />&lt;value&gt;true&lt;/value&gt;<br />&lt;description&gt;The mode the cluster will be in. Possible values are<br />false: standalone and pseudo-distributed setups with managed Zookeeper<br />true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;hbase.master.info.port&lt;/name&gt;<br />&lt;value&gt;60010&lt;/value&gt;<br />&lt;description&gt;The port for the hbase master web UI<br />Set to -1 if you do not want the info server to run.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;<br />&lt;value&gt;2222&lt;/value&gt;<br />&lt;description&gt;Property from ZooKeeper's config zoo.cfg.<br />The port at which the clients will connect.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;property&gt;<br />&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;<br />&lt;value&gt;hdp-cl01,hdp-cl02,hdp-cl03,hdp-cl04&lt;/value&gt;<br />&lt;description&gt;Comma separated list of servers in the ZooKeeper Quorum.<br />For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".<br />By default this is set to localhost for local and pseudo-distributed modes<br />of operation. For a fully-distributed setup, this should be set to a full<br />list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh<br />this is the list of servers which we will start/stop ZooKeeper on.<br />&lt;/description&gt;<br />&lt;/property&gt;<br />&lt;/configuration&gt;<br /><br /></pre>
<p>Mit Hbase haben wir gleichzeitig ZooKeeper installiert und konfiguriert. Waehrend Hadoop mit unserem Map/Reduce-Cluster nur Jobs ausfuehren kann, die beliebig lange dauern koennen, haben wir mit Hbase eine Datenbank vorliegen,<br />die wir auch in Echtzeit abfragen koennen.<br />Dazu starten wir mit</p>
<blockquote>
<p>/home/hadoop/hbase/bin/start-hbase.sh</p>
</blockquote>
<p>alle Dienste. Mit der hbase-Shell koennen wir eine Tabelle mit 3 Spalten in Hbase anlegen:</p>
<p> </p>
<blockquote># /home/hadoop/hbase/bin/hbase<br />hbase(main):001:0&gt; create 'my_table', 'f1', 'f2', 'f3'<br />0 row(s) in 0.1600 seconds<br />hbase(main):002:0&gt; list<br />my_table</blockquote>
<p> </p>
<p><br />Wir starten in Hbase Stargate und stellen somit eine REST-Schnittstelle im Web zur Verfuegung:</p>
<p> </p>
<blockquote># /home/hadoop/hbase/bin/hbase-daemon.sh start org.apache.hadoop.hbase.stargate.Main -p 8090<br /></blockquote>
<p> </p>
<p>Wenn wir jetzt im Browser die Adresse http://hdp-cl01.mydomain.de:8090/ ansurfen, sollten wir dort "mytable" lesen, was<br />dem ersten GET-Aufruf unserer REST-Schnittstelle betrifft. Mit GET, POST, PUT, DELETE koennen wir unsere nosql db<br />weiter bedienen.</p>
<p><br />Fassen wir noch einmal zusammen:<br />- Wir haben einen Cluster gebaut, auf dem wir Rechenleistung auf mehrere Rechner verteilen koennen.<br />- Wir haben ein Cluster-Filesystem mit einer zweifachen Redundanz aller Daten.<br />- Wir haben eine Datenbank auf mehreren Knoten, die wir uebers Web bedienen koennen.</p>
<p>Was machen wir damit? Nach einigen Ueberlegungen fallen einem noch paar mehr Sachen ein ausser<br />Logfileauswertung mit Map/Reduce:<br />Variante 1: Content-Management-System (CMS).<br />Content wird sowieso als xml-Files angeliefert. Wir bilden das XML-Schema im Hbase nach und fuettern die Hbase-<br />Datenbank mit den Content-Artikel. Aufrufen kann man die Artike sofort ueber ihre row-Number in der Datenbank etwa<br />ueber http://hdp-cl01.meinedomain.de:8090/cnt/001. Eigentlich brauch man bloss noch eine XHTML-Seite dazu und<br />vielleicht moechte man auch noch nach Artikeln suchen oder diese in einer Ausgabe nach Themen sortieren. Muss man<br />jetzt bauen? Gibt es schon! Lily-CMS auf http://www.lilycms.org/lily/index.html setzt auf HBase auf und beabsichtigt ein<br />Content-Repository-System mit einigen Hilfsmitteln zu erstellen.<br />Das Projekt ist in der Alpha-Phase und man darf gespannt sein bis zum Beta-Release.<br />Variante 2: Webmail-System (SMTP). From:<br />Es ist nicht mehr zeitgemaess, seine Emails mit einem externen Mailprogramm wie Outlook oder Thunderbird zu<br />bearbeiten. Langfristig gesehen haben Applikationen, fuer die ein Extra-Programm zur Bedienung notwendig sind ausser<br />der Webbrowser, keine Zukunft. Egal ob es Chat, Foren, Mail oder die Administration der Homepage oder des Blogs ist<br />("Hallo, Sie baden gerade Ihre Finger dadrin"), alles wird oder ist bereits ueber den normalen Webbrowser bedienbar -<br />einfach und von ueberall.<br />Wenn man jetzt mal eine Email betrachtet, so besteht diese aus bestimmten Objekten:<br />To:<br />Subject:<br />Body<br />Normalerweise sind diese Objekte Bestandteil eines Files, welches als Datei in einem Directory oder mit vielen anderen<br />Objekten in einer Datei beherbergt wird - je nachdem ob man ein maildir oder mailbox Mailsystem betreibt. Je nach<br />Filesystem ergeben sich dann physikalische Grenzen beim Zugriff auf die Mails, deren Ursache einfach in der Architektur<br />zu finden sind. Beim Zugriff auf das Directory liest ein Prozess alle Dateinamen ein, um eine Uebersicht der Mails zu<br />generieren. Beim Zugriff auf ein File wird das komplette File eingelesen, um daraus dann eine Uebersicht der Mails zu<br />generieren.<br />WAS WAERE JETZT, WENN DIESE OBJEKTE INHALT EINES SCHEMAS EINER DATENBANK WAEREN???<br />Ein Wahnsinn, ich weiss. Aber auch darueber hat sich schon jemand Gedanken gemacht.<br />Edward J. Yoon beschaeftigt sich im Hama-Projekt mit mathematischen und graphischen Berechnung rund um Grid-<br />Computing. Er hat eine Studie erstellt, wie Grid-Computing in ein Webmail-System zu integrieren ist.<br />In Taiwan fand im Mai 2010 die CBT 2010 (Grid and Pervasive Computing) statt. Dort gab es einen Vortrag von Duy<br />Phuong Pham mit dem Thema A Fully-Protected Large-Scale Email System Built on Map-Reduce Framework. (auch<br />erschienen unter Springer Verlag http://www.springerlink.com/content/xu46250m2267877n/ auszugsweise ab S. 662<br />unter <a href="http://books.google.com/books?id=HyFNWR1mCucC&lpg=PR5&dq=978-3-642-13066-3&hl=de&pg=PA662#v=onepage&q=978-3-642-13066-3&f=false" target="_blank">books.google.com</a> als Download auch <a href="http://www.eumel.de/images/stories/paper.pdf" target="_blank">hier</a>) . Neben dem Abspeichern der Mails im HDFS-Filesystem wird<br />insbesondere noch auf die Spam-Erkennung mittels Map/Reduce eingegangen. Eine sehr interessante Ueberlegung,<br />wenn auch erstmal nur in der Theorie an der Universitaet Taiwan.<br />Aus heutiger Sicht leider auch technologisch schon 3 Schritte weiter.</p>
<p>- <a href="http://hadoop.apache.org/hdfs/" target="_blank">Hadoop</a><br />- <a href="http://hadoop.apache.org/hdfs/" target="_blank">HDFS</a><br />-<a href="http://hbase.org/" target="_blank"> Hbase</a><br />- <a href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/stargate/package-summary.html#package_description" target="_blank">Stargate</a></p>
